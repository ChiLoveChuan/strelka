\documentclass{article}

\usepackage{natbib}

% for equation*
\usepackage{amsmath}

% for scalebox,...
\usepackage{graphics}

% hide hyperref links  with pdfborder (more portable than hidelinks option)
\usepackage[pdfborder={0 0 0}]{hyperref}

% for pseudocode
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}


\title{'Methods for Strelka Small Variant Caller'}


% simple scientific notation:
\newcommand{\e}[1]{\ensuremath{\times 10^{#1}}}

\begin{document}

\maketitle

\tableofcontents

\section{Overview}

On any release branch, the methods described here should reflect the default implementation in the source repository containing this document. Methods are written to an audience and level of detail similar to that provided in a journal methods writeup. Additional \emph{design discussion} sections are added to comment on why a method was chosen or in what way it could be improved.

\section{Methods}

\subsection{Workflow Overview}

Strelka comprises several workflows to call small variants from mapped sequencing data. Supported small variant calling applications include detection of germline variants in a set of samples, somatic variants from matched tumor-normal sample pairs and denovo variants from parent-offspring trios.

The methods below describe several core components shared by all application workflows, in addition to model customizations made for each type of variant calling problem. All workflows share a common sequence of analysis steps:

\begin{enumerate}
\item Preliminary estimation of parameters from sample data.
\item Division of the genome into segments to be analyzed in parallel
\item For each analyzed genome segment:
\begin{enumerate}
\item Filtration of input read alignments
\item Processing the input read alignments into a set of variant haplotype candidates.
\item Finding probability of variant haplotypes/haplotype combinations under various (germline, somatic, de-novo) models.
\item Empirical re-scoring of variants based on models trained from static truth sets.
\end{enumerate}
\item Joining parallel analysis results into final reported output format.
\end{enumerate}

The details of the shared and application specific model steps are provided below.


\subsection{Preliminary Parameter Estimation}

An initial step in all workflows is the rapid estimation of the sequencing depth for each chromosome. For somatic analysis this depth is only computed for the normal sample. This information is used downstream to filter out high-depth regions (details below) when Strelka is run in its default whole genome sequencing mode. This step is skipped for exome or other targeted analyses.

\subsubsection{Chromosome depth estimation}

For each chromosome, depth is estimated using a modified median calculation. As a first step, each chromosome is partitioned into segments of similar size to ensure the estimation process samples several chromosome regions. The chromosome is divided into the smallest number of segments no larger than $S_{max}$, where all segments have nearly equal size (all sizes must be $S$ or $S+1$, given smallest segment size of $S$). If this procedure results in more than 20 segments then $S_{max}$ is doubled until 20 or fewer segments are found. $S_{max}$ is initialized to 2 Mbase.

The depth estimation procedure repeatedly cycles through all segments. Within each segment, at least 40,000 reads are scanned before moving to the next segment. Whenever a read is scanned at a given mapping position, additional reads are scanned until the mapping position changes. After the target number of reads have been scanned from every segment in the chromosome, the procedure returns to the first position and repeats this procedure starting from the last unscanned position. The process repeats until the all reads in all segments are scanned or the depth estimate converges.

Each scanned read is filtered if it is unmapped. Otherwise the read alignment is ignored and the read is applied to a simple depth pileup assuming a complete and ungapped alignment starting from the mapping position. Every 1M reads triggers a convergence check, but only after every chromosome segment has been sampled at least once.

Depth is estimated from the resulting pileup, using a simple median over all depth observations from the pileup structure excluding zero counts. Convergence (defined as an absolute change of less than 0.05 or, in the median case, as an exact match) is checked between the depth estimate of the last convergence check and the current one.

The depth estimation procedure is run separately for each non-tumor sample, and all high-depth thresholds are set based on the sum of depth estimates over these samples.

\subsubsection{Depth estimation design discussion}

The objective of the depth estimation step is to find the expected depth and/or depth distribution for each copy number state or plasmid -- at present dividing this estimation to run separately on each chromosome is a reasonable approximation towards this goal. An improvement would be to estimate the depth distribution properties jointly over each copy number state and plasmid.

\subsection{Small Variant Discovery}


\subsubsection{Short Haplotyping}

Strelka processes dense variants using ``short haplotyping". The short haplotyping is conceptually similar to the haplotype based calling used in GATK and Platypus in that dense SNPs and indels are called simultaneously through haplotype reconstruction. But in contrast to GATK and Platypus that use local assembly to generate haplotypes longer than reads, Strelka generates short haplotypes ($\leq$ 30 bp) using simple read counting. The short haplotyping is composed of 3 steps: (1) detecting short regions with dense variants called {\em active regions}, (2) creating candidate haplotypes, and (3) aligning each haplotype to the reference to discover primitive alleles (SNVs and/or indels). 

\paragraph{Active region detection}
For each reference position, Strelka retains a variant counter. Variants in reads are parsed from the input BAM file and increase the variant counters in corresponding positions. A mismatch at position $i$ increases the counter at $i$ by 1. An insertion at position $i$ increases the counter at $i$ and $i+1$ by 4. A deletion at positions $[i,j]$ increases the counters in $[i-1,j]$ by 4. 
After all the reads overlapping a position $i$ are parsed, if the variant count at $i$ is equal or larger than 9, $i$ becomes a {\em candidate variant position}.

Strelka employs an active region detector to detect active regions. The active region detector holds an active region start position ($i_{start}$) and a previous candidate variant position ($i_{prev}$), where both are initialized as 0. When a new candidate variant position $i$ is found, it extends the existing active region if it is not far from the previous variant (i.e. $i - i_{prev} \leq 14$) and including $i$ does not make the active region long (i.e. $i - i_{start} \leq 30$). For consecutive variants (i.e. $i - i_{prev} = 1$), $i$ extends the existing active region even if $i - i_{start} > 30$. If $i$ extends the existing active region, the active region detector simply updates $i_{prev} = i$. Otherwise, the active region detector creates a new active region $[i_{start},i_{prev}]$ if $i_{start} < i_{prev}$ and starts a new active region by setting $i_{start}=i_{prev}=i$. 

\paragraph{Haplotype generation}
Once an active region is created, the active region detector generates candidate haplotypes of the region. It considers reads that fully span the region and identifies subsequences corresponding to the region. Also for each unique subsequence, the count of reads having the subsequence is also calculated. The subsequence becomes a candidate haplotype if the read count is among the top 3 most read counts and also no less than 3.

\paragraph{Primitive allele discovery}
Candidate haplotypes are aligned to the reference sequence of the active region. From the alignment, primitive alleles (SNVs and indels) are identified. These primitive alleles are used to improve SNV and indel calling by (1) relaxing the mismatch density filter (described in \ref{ReadFiltration}) and (2) improving indel candidacy (described in \ref{IndelCandidacy}).

\subsubsection{Read Filtration} \label{ReadFiltration}

{\tt TODO: description of the mismatch density filter (MMDF)}
%Strelka filters out fractions of reads, where the densities of mismatches and indels are high.

If short haplotyping is enabled, SNVs discovered in active regions are used to relax the MMDF. For this, the active region detector records all the SNV positions discovered in active regions separately for each sample. When the mismatch density is calculated for a read, any mismatch at position $i$ is exempted from measuring the mismatch density if $i$ is a recorded SNV position for the sample, i.e. MMDF filters out bases if there are dense {\em inconsistent} variants around.

\subsubsection{Indel Candidacy} \label{IndelCandidacy}

Strelka uses indel candidacy as a preliminary filter to eliminate indel observations from consideration as variants if they are very likely to have been generated by error processes.  A candidate indel variant must minimally have 2 reads supporting it in at least one sample (this is relevant in multi-sample settings, such as somatic, trio de novo, and germline multi-sample calling). If short haplotyping is enabled, a candidate indel must also be discovered in an active region in at least one sample. If an indel observation satisfies this minimal condition, Strelka evaluates its candidacy status using a one-sided binomial exact test, with the null hypothesis being that the indel coverage is generated by indel error processes.

Given a total locus coverage of $N$, indel coverage of $n_i$, and an indel error rate of $e_l$ (described below in ``\nameref{sec:indel_error}''), we define the probability of some coverage $x$ being generated at the locus due to indel error as
\begin{equation*}
P(x \mid N, e_l) = \binom {N} {x} e^{x}_l (1 - e_l)^{N - x}
\end{equation*}
We can then define the probability of generating an indel with at least coverage $n_i$ due to indel error at a locus as follows:
\begin{equation*}
p_{error} = P(X >= n_i \mid N, e_l) = \sum_{x = n_i}^{N} P(x \mid N, e_l)
\end{equation*}

\noindent The indel is considered as a candidate variant if $p_{error} < p_{reject}$, where $p_{reject}$ is the rejection threshold for the null hypothesis. The default value $p_{reject}$ is $1\e{-9}$.

\subsubsection{Read Realignment}

\subsection{Small Variant Scoring}

\subsubsection{Core Likelihood Function}

\paragraph{Indel Error Model}
\label{sec:indel_error}
Indel errors are approximated as a process which occurs independently in each read, with some fixed probably of an indel error occurring as a function of the local sequence STR context. Strelka's indel error parameters are currently set based on empirical observation of indel calling performance for both germline and somatic indel detection.

The default indel error rates used by strelka are a function of the local homopolymer context length $l_{\text{STR}}$, so any expansion or contraction of a homopolymer sequence has an error rate indexed on the homopolymer length in the presumed source haplotype upon which the indel error process occurred. All other types of indels, including dinucleotide tract indels and mutations of homopolymers which do not represent simple expansion/contraction events, have $l_{\text{STR}}$ set to one. By observation, the indel error rates are set to a log-linear ramp as a function of $l_{\text{STR}}$

\begin{equation*}
P(error \mid l_{\text{STR}}) = e_{l} \exp(f_{\text{STR}}(\log(e_{h})-\log(e_{l})))
\end{equation*}

\noindent where the constant indel errors set for low and high STR lengths are $e_{l} = 5\e{-5}$ and $e_{h} = 3\e{-4}$, and the fraction of high STR length is $f_{\text{STR}} = \max((l_{\text{STR}}-1),15)/15$.

As a special case, the germline indel genotyping model uses higher error rates to heuristically account for assembly and mapping errors. This is currently a scaling factor of 100, $P(error_{\text{germline}} \mid l_{\text{STR}}) = 100 * P(error \mid l_{\text{STR}})$


\subsubsection{Somatic Calling Model and Variant likelihoods}

The somatic calling model assumes that the samples are from diploid individuals. For both SNVs and indels, the normal genotype state space is the set of unphased diploid genotypes, $G_n = \{ \texttt{ref}, \texttt{het}, \texttt{hom}\}$, where $\texttt{ref}$, $\texttt{het}$, and $\texttt{hom}$ refer to the normal genotype being reference, heterozygous, and homozygous, respectively. The tumor genotype has two states $G_t = \{ \texttt{nonsom}, \texttt{som} \}$, where $\texttt{nonsom}$ and $\texttt{som}$ indicate the absence and presence of somatic mutation in the tumor sample, respectively. The method approximates a posterior probability on the joint tumor and normal genotypes.

\begin{align*}
	& P(G_t,G_n \mid D) \propto P(G_t,G_n) P(D \mid G_t,G_n) \\
\end{align*}


Here $D$ refers to the sequencing data from both samples. The likelihood term above is computed from independent sample-specific genotype likelihoods

\begin{align*}
	& P(D \mid G_t,G_n) = \int_{F_t,F_n}{P(D \mid F_t,F_n)P(F_t,F_n \mid G_t,G_n)} \\
	& = \int_{F_t,F_n}{P(D_t \mid F_t)P(D_n \mid F_n)P(F_t,F_n \mid G_t,G_n)},
\end{align*}

\noindent where $F_t$, $F_n$ refer to tumor and normal allele frequencies and $D_t$ and $D_n$ indicate tumor and normal sample data. In the above equation, the sample-specific allele frequency likelihoods $P(D_t \mid F_t)$ and $P(D_n \mid F_n)$ could in principle be supplied by any probabilistic variant caller. Strelka currently provides its own sample-specific frequency likelihoods (described in Section xxx). The genotype prior probability $P(G_t, G_n)$ and the joint allele-frequency distribution $P(F_t,F_n \mid G_t,G_n)$ are detailed below.

The posterior probability over tumor and normal genotypes $P(G_t,G_n \mid D)$ is used to compute the {\em somatic variant probability} (QSS for SNVs and QSI for indels).

\begin{equation}
\label{eqn:somVarProb}
	P(G_t = \texttt{som} \mid D) = \sum_{G_n \in \{ \texttt{ref}, \texttt{het}, \texttt{hom} \}}{P(G_t=\texttt{som},G_n \mid D)}.
\end{equation}

As previously discussed, this somatic variant probability is not ideal for detecting variants of interest because it does not distinguish somatic variant types. We therefore associate somatic calls with the probability of somatic variation {\em and} the normal sample genotype being $\texttt{ref}$ (QSS\_NT for SNVs and QSI\_NT for indels), i.e., $P(G_t = \texttt{som}, G_n = \texttt{ref} \mid D)$. All Strelka quality scores discussed below express these values.

The Strelka workflow uses two calling tiers. All somatic calls are classified according to their most-likely normal genotype if that value is the same in tiers 1 and 2, and classified as conflicts otherwise.

\paragraph{Genomic prior}
Given the expected rate of variants between two unrelated haplotypes (Watterson theta) $\theta$, the normal genomic prior $P(G_n)$ over the set of diploid genotypes is

\begin{equation*}
P(G_n)=
\begin{cases}
	\theta & \text{if } G_n = \texttt{het} \\
	\theta/2 & \text{if } G_n = \texttt{hom} \\
	1 - 3\theta/2 & \text{if } G_n = \texttt{ref} \\
\end{cases}
\end{equation*}

\noindent where $\theta_{\text{SNV}}=1\e{-3}$ for SNVs and $\theta_{\text{indel}}=1\e{-4}$ for indels. Given the somatic state prior $P(G_t=\texttt{som}) = \gamma$, the genomic prior is

\begin{equation*}
P(G_t, G_n)=
\begin{cases}
	(1 - \gamma) P(G_n) & \text{if } G_t = \texttt{nonsom} \\
	\gamma P(G_n) & \text{if } G_t = \texttt{som} \\
\end{cases}
\end{equation*}

\noindent where $\gamma$ is set to $\gamma_{\text{SNV}} = 1\e{-3}$ and $\gamma_{\text{indel}} = 1\e{-6}$ for SNVs and indels. Note that $\gamma$ is expected to scale the somatic variant probabilities but not substantially influence their rank, thus its value was chosen empirically to provide reasonable variant probabilities and are not adjusted for different samples in practice.


\paragraph{Joint allele-frequency prior}
The prior probability on the tumor and normal allele-frequencies $P(F_t, F_n \mid G_t, G_n)$ encodes the concept that the normal sample is a mixture of diploid germline variation and noise while the tumor sample is a mixture of the normal sample and somatic variation. Let $\mathcal{C} (F_n, G_n) = 1$ if $F_n$ is a {\em canonical} diploid allele frequency of $G_n$ and $\mathcal{C} (F_n, G_n) = 0$ otherwise. For example, $\mathcal{C} (F_n=0, G_n = \texttt{ref}) = 1$ and $\mathcal{C} (F_n=0.4, G_n = \texttt{ref}) = 0$. The joint frequency prior is defined as follows.

% Non-somatic
\begin{equation*}
P(F_t, F_n \mid G_t = \texttt{nonsom}, G_n)=
\begin{cases}
	0 & \text{ if } F_t \neq F_n \\
	1-\mu & \text{ if } F_t = F_n \text{ and }\mathcal{C}(F_n, G_n) = 1 \\
	\mu U(F_t) & \text{ if } F_t = F_n \text{ and }\mathcal{C}(F_n, G_n) = 0 \\
\end{cases}
\end{equation*}

% Somatic, normal genotype ref
\begin{equation*}
P(F_t, F_n \mid G_t = \texttt{som}, G_n = \texttt{ref})=
\begin{cases}
	(1-\mu)U(F_t) & \text{ if } F_t \neq F_n \text{ and } F_n \leq \tau F_t \\
	0 & \text{ otherwise } \\
\end{cases}
\end{equation*}

% Somatic, normal genotype het or hom
\begin{equation*}
P(F_t, F_n \mid G_t = \texttt{som}, G_n \neq \texttt{ref})=
\begin{cases}
	(1-\mu)U(F_t) & \text{ if } F_t \neq F_n \text{ and } \mathcal{C}(F_n, G_n) = 1 \\
	0 & \text{ otherwise } \\
\end{cases}
\end{equation*}

\noindent Here, $\tau$ represents the {\em contamination tolerance}, $U(F_t)$ refers to a uniform distribution over the allowed tumor allele frequencies and $\mu$ indicates the noise term. The contamination tolerance term is introduced to allow for contamination in the normal sample by some fraction of tumor cells. This is particularly useful for analyses of liquid tumors, where normal sample is almost always contaminated by tumor cells. The contamination tolerance is set to $0.15$. The noise term abstracts various sequencing, read mapping and assembly issues which could produce an unexpected allele frequency shared in the tumor and normal samples. For SNVs, the noise contribution is set to $\mu_{\text{SNV}} = 5\e{-10}$. For indels, it is dynamically estimated as described in Section xxx.

% The description of the strand-bias model is not included here because it is not used in the quality score calculation.

\paragraph{Practical computation}

The continuous allele frequencies modeled above are efficiently computed by dividing each allele-pair axis into a set of equidistant points and performing the somatic probability computation over the resulting discrete point set. Several point resolutions were attempted to confirm the expected stability and convergence of results as resolution increased. A resolution of 21 points per axis is used for all computations by default. We expect that this resolution should be increased for improved detection of somatic allele frequencies lower than 5\%.

\paragraph{Somatic callability track}

The somatic variant calling workflow optionally provides a somatic callability track expressing, for each reference position, whether there is sufficient sequencing evidence to confidently assert either (1) the presence of a somatic SNV or (2) the absence of a somatic SNV with somatic variant frequency of 10\% or higher. Evidence for the presence of a somatic SNV is quantified by the QSS score described above in equation \ref{eqn:somVarProb}. Evidence for the absence of a somatic SNV is quantified by the \emph{non-somatic SNV probability} (or the equivalent phred-scaled score, NQSS) described below. A reference position is marked as "callable" in the somatic callability track if QSS $\ge$ 15 or NQSS $\ge$ 15.

The NQSS score is computed from the same sample-specific allele frequency likelihoods, $P(D_t \mid F_t)$ and $P(D_n \mid F_n)$, used to compute QSS; however, the genotype prior probability $P(G_t, G_n)$ and the joint allele-frequency distributions $P(F_t,F_n \mid G_t,G_n)$ are adjusted as described below.

The diploid genotype prior used for the NQSS score is

\begin{equation*}
P(G_n\mid \text{NQSS})=
\begin{cases}
0 & \text{if } G_n = \texttt{het} \\
1/2 & \text{if } G_n = \texttt{hom} \\
1/2 & \text{if } G_n = \texttt{ref} \\
\end{cases}
\end{equation*}

The prior distribution over somatic genotypes is uniform, thus the full genotype prior used for the NQSS score is

\begin{equation*}
P(G_t, G_n \mid \text{NQSS}) =
\begin{cases}
P(G_n \mid \text{NQSS})/2 & \text{if } G_t = \texttt{nonsom} \\
P(G_n \mid \text{NQSS})/2 & \text{if } G_t = \texttt{som}. \\
\end{cases}
\end{equation*}


The joint allele-frequency distribution used for the NQSS score is

\begin{equation*}
P(F_t, F_n \mid G_t = \texttt{nonsom}, G_n, \text{NQSS})=
\begin{cases}
U(F_t) & \text{ if } F_t = F_n \\
0 & \text{ otherwise } \\
\end{cases}
\end{equation*}

\begin{equation*}
P(F_t, F_n \mid G_t = \texttt{som}, G_n, \text{NQSS})=
\begin{cases}
U(F_t)/2 & \text{ if } F_t \neq F_n, G_n \in \{\texttt{ref},\texttt{hom}\}  \\
0 & \text{ otherwise } \\
\end{cases}
\end{equation*}

Note that the somatic callability track is based on the core somatic variant quality model and does not reflect additional information about each candidate variant integrated into the empirical variant scoring step described below. For this reason the somatic calling track will diverge from Strelka's final somatic SNV output in some cases.



\subsubsection{Somatic Filtration and Scoring}

\subsubsection{Germline Calling Model and Genotype likelihoods}

\subsubsection{Germline Filtration and Scoring}
The final score of a candidate variant is assigned by a classifier of which the input features are the variant probability described above along with several other features that may not be taken into account fully in the probabilistic model. We refer to this step as \emph{empirical variant scoring}.

\paragraph{Model description, training, and calibration}
Empirical variant scoring in Strelka is performed by two random forest classifiers -- one for snvs and one for indels -- as implemented in the scikit-learn python package, trained on a labeled set of candidate variants. Each classifier uses 50 decision trees with a maximum depth of 12, a minimum of 50 samples per leaf, and no limit on the maximum number of features.

The training dataset is compiled from a collection of sequencing runs on different platforms and chemistries, all from an individual for which a gold standard truth set is available via the Platinum Genomes project. Candidate variants that correspond to the high-confidence regions of the truth set are labeled as true or false using the hap.py haplotype comparison tool. Variants that exist in the truth set but were called with incorrect genotype are treated as false variants. It is believed that the vast majority of candidate variants outside of the high-confidence regions (and classified by hap.py as unknown) are false; for this reason, these variants are added to the set of false variants that are presented to the model during training.

The output scores produced by the random forest classifier are well calibrated under the assumption (used during training) that all of the unknown candidate variants are false. However, if this assumption is not made and the model performance is evaluated on variants in the confident regions only, the model is overly pessimistic. In order to achieve an acceptable precision-recall tradeoff, the model output is recalibrated by passing it through a monotonic transform that is set to roughly optimize f-score when the resulting score is thresholded at the default phred-scale threshold of 15.

\paragraph{Feature descriptions}

\subsubsection{Denovo Calling Model and Genotype likelihoods}

\subsubsection{Denovo Filtration and Scoring}


\bibliographystyle{alpha}
\bibliography{methods}

\end{document}
